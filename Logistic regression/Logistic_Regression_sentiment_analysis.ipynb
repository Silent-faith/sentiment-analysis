{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression sentiment analysis",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0WObTWpc89"
      },
      "source": [
        "# Helper function used for the text preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3oSPcDJjg1T"
      },
      "source": [
        "import numpy as np \n",
        "import string \n",
        "import re \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def process_sentence(sentence) :\n",
        "    \"\"\"\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : a string of words \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    clean_sentence : a string of words without having the unessasry words \n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    sentence = re.sub(r'^RT[\\s]+', '', sentence)\n",
        "    # remove hyperlinks\n",
        "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    sentence = re.sub(r'#', '', sentence)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
        "    \n",
        "    sentence_tokens = tokenizer.tokenize(sentence)\n",
        "    \n",
        "    clean_sentence = []\n",
        "    for word in sentence_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            \n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            clean_sentence.append(stem_word)\n",
        "    \n",
        "    return clean_sentence\n",
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of sentences\n",
        "        ys: an m x 1 array with the sentiment label of each sentence\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_sentence(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpKsrKQyVupl"
      },
      "source": [
        "# Importing the libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgAuMbEFVzzM"
      },
      "source": [
        "import numpy as np \n",
        "import re \n",
        "import string \n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "import nltk"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQZ7OF01XIU4"
      },
      "source": [
        "# Downloading the required data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwgYAT02W4Q3",
        "outputId": "e1a4df8d-e83d-405c-bd6a-3619e704792b"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLNq0cIMWeQ3"
      },
      "source": [
        "# Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yiLGvwBWa69"
      },
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y_-jPi2XTpg",
        "outputId": "8a10d965-45ea-4551-b890-5a975d8935f0"
      },
      "source": [
        "print(len(all_positive_tweets), len(all_negative_tweets))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg5wezb9XcfD"
      },
      "source": [
        "# split the independent data (train_X and test_X) into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwGtdq_hXvI7",
        "outputId": "3117c584-211b-4170-b872-db60aabb4072"
      },
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFFpHMayX6ev"
      },
      "source": [
        "# Pre processing the Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5vRySUqX52e",
        "outputId": "14e8ebec-4b0a-4876-f524-3ea8e5f4006c"
      },
      "source": [
        "\"\"\"\n",
        "Here in this build_freqs function we have done following things \n",
        "\n",
        "1) At first we remnoved all the stopwords from the tweets\n",
        "2) After that we stem down all the words to there stem words so that the length of frquency table is small and all similar words could be grouped \n",
        "3) After that we have built a frequency table for all the words in the sentence\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "word_frequency = build_freqs(train_x, train_y)\n",
        "\n",
        "print(\"lenght of the frequency table\", len(word_frequency))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lenght of the frequency table 11346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI2olkb6ZiAj"
      },
      "source": [
        "# Constructing the model for the classification of the tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHNaWwusZheI"
      },
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ikwTqcMkMxA"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
        "        \n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgHQI-G2m2r2"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "\n",
        "    word_l = process_sentence(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JZA7-aynPJr"
      },
      "source": [
        "# Training the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4zIi6AUnSDZ",
        "outputId": "d51f8ca0-25ff-4a00-b675-5206a2a99744"
      },
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], word_frequency)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cost after training is 0.24216529.\n",
            "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnCzBL5QnlgH"
      },
      "source": [
        "# testing of the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIZAoGqcnlDb"
      },
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYTnkNpdnyrQ",
        "outputId": "11ba8cfd-4f80-4a60-cf4d-01421e30b5d3"
      },
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, word_frequency, theta)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am happy -> 0.518580\n",
            "I am bad -> 0.494339\n",
            "this movie should have been great. -> 0.515331\n",
            "great -> 0.515464\n",
            "great great -> 0.530898\n",
            "great great great -> 0.546273\n",
            "great great great great -> 0.561561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg3eAfp0n-56"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thajs-PLn-lx"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu3GnnaroG8W",
        "outputId": "a4ce3c3c-8146-4a2e-9c8a-b6a60e65b3c0"
      },
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, word_frequency, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu4Yq8icoPI8"
      },
      "source": [
        "# Predicting your own tweet \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FboutrREoOQp",
        "outputId": "1720e035-a60b-4cdf-f314-52602ef17a9c"
      },
      "source": [
        "my_tweet = input(\"please enter your tweet or sentence\")\n",
        "#print(process_sentence(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, word_frequency, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please enter your tweet or sentenceIt was good to work with this project and I loved it \n",
            "[[0.53740669]]\n",
            "Positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}